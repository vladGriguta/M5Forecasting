{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit to [ragnar](https://www.kaggle.com/ragnar123). This notebook builds upon their data pipeline found in  [ragnar's notebook](https://www.kaggle.com/ragnar123/very-fst-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial notebook:\n",
    "1. Read in data from calendar, sell_prices, sales_train_validation, submission and apply common method to reduce the memory usage\n",
    "2. Merge data into a multivariate time series (MTS) format with each entry in the series describing the target variable (demand) and its exogeneous variables in different dates for different item_id, dept_id, cat_id, store_id and state_id\n",
    "3. Transform features into the MTS (fill NaN events with category 'unknown' and encode all categorical variables)\n",
    "4. Engineer additional features based on the series of prices and demand for each good. Examples include aggregations over lagged sub-series of the two variables\n",
    "### Personal contributions (thus far)\n",
    "5. Hand-crafted parameters for the lightGBM regressor based upon [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)\n",
    "6. Implemented hyper-parameter tuning pipeline for optimizing the n_estimators, max_depth, num_leaves, learning_rate\n",
    "### Future plans\n",
    "7. Find a way to read in the full dataset and avoid memory problems\n",
    "8. Implement a feature selection method (sklearn.feature_selection)\n",
    "9. Engineer additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import json\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common function to reduce the memory usage thus allowing us to work with larger datasets\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Common function to reduce the size of the entries in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# simple function to read the data in the competition files\n",
    "def readData():\n",
    "    import pandas as pd\n",
    "    print('Reading files...')\n",
    "    calendar = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "    \n",
    "    sell_prices = pd.read_csv('m5-forecasting-accuracy/sell_prices.csv')\n",
    "    sell_prices = reduce_mem_usage(sell_prices)\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "    \n",
    "    sales_train_validation = pd.read_csv('m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    \n",
    "    \n",
    "    submission = pd.read_csv('m5-forecasting-accuracy/sample_submission.csv')\n",
    "    \n",
    "    return calendar, sell_prices, sales_train_validation, submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Sell prices has 6841121 rows and 4 columns\n",
      "Sales train validation has 30490 rows and 1919 columns\n"
     ]
    }
   ],
   "source": [
    "calendar, sell_prices, sales_train_validation, submission = readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data to get it into a tabular format; pd.melt is especially useful to 'unpack' the target variable (demand)\n",
    "def melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 55000000, merge = False):\n",
    "    \n",
    "    # melt sales data, get it ready for training\n",
    "    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                                     var_name = 'day', value_name = 'demand')\n",
    "    \n",
    "    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "    \n",
    "    # seperate test dataframes\n",
    "    test1_rows = [row for row in submission['id'] if 'validation' in row]\n",
    "    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    test1 = submission[submission['id'].isin(test1_rows)]\n",
    "    test2 = submission[submission['id'].isin(test2_rows)]\n",
    "    \n",
    "    # change column names\n",
    "    test1.columns = ['id'] + ['d_{}'.format(i) for i in range(1914,1942)]\n",
    "    test2.columns = ['id'] + ['d_{}'.format(i) for i in range(1942,1970)]\n",
    "\n",
    "\n",
    "    # get product table\n",
    "    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "    \n",
    "    # merge with product table\n",
    "    test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n",
    "    test1 = test1.merge(product, how = 'left', on = 'id')\n",
    "    test2 = test2.merge(product, how = 'left', on = 'id')\n",
    "    test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n",
    "    \n",
    "    # \n",
    "    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day',\n",
    "                    value_name = 'demand')\n",
    "    test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day',\n",
    "                    value_name = 'demand')\n",
    "    \n",
    "    sales_train_validation['part'] = 'train'\n",
    "    test1['part'] = 'test1'\n",
    "    test2['part'] = 'test2'\n",
    "    \n",
    "    data = pd.concat([sales_train_validation, test1, test2], axis = 0)\n",
    "    \n",
    "    del sales_train_validation, test1, test2\n",
    "    gc.collect()\n",
    "    \n",
    "    # get only a sample for fst training\n",
    "    data = data.loc[nrows:]\n",
    "    \n",
    "    # drop some calendar features\n",
    "    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n",
    "    \n",
    "    # delete test2 for now\n",
    "    data = data[data['part'] != 'test2']\n",
    "    \n",
    "    if merge:\n",
    "        # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n",
    "        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "        data.drop(['d', 'day'], inplace = True, axis = 1)\n",
    "        # get the sell price data (this feature should be very important)\n",
    "        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melted sales train validation has 58327370 rows and 8 columns\n",
      "Mem. usage decreased to 3226.27 Mb (9.4% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final dataset to train has 31681090 rows and 18 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 2.75e7, merge = True)\n",
    "del calendar, sell_prices, sales_train_validation\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function fills up the Nan values and encodes the categorical variables\n",
    "def transform(data):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    # convert to datetime object\n",
    "    data['date'] = pd.to_datetime(data.date)\n",
    "    \n",
    "    # fill NaN features with unknown\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "    \n",
    "    return data\n",
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1.783752e+06 / 3.168109e+07 NaN entries in the sell_price column\n"
     ]
    }
   ],
   "source": [
    "print('There are {:e} / {:e} NaN entries in the sell_price column'.format(data.sell_price.isna().sum(),data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes useful laging features from the target variable and the price\n",
    "# to convert what is initially a sequence to sequence mapping into a regression task (sequence to one),\n",
    "# the author used lagged values starting from the minimum lag of 28 days, which is the forecasting horizon\n",
    "def simple_fe(data):\n",
    "    \n",
    "    # rolling demand features\n",
    "    data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n",
    "    data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n",
    "    data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n",
    "    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "    \n",
    "    \n",
    "    # price features\n",
    "    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) / (data['lag_price_t1'])\n",
    "    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) / (data['rolling_price_max_t365'])\n",
    "    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n",
    "    \n",
    "    # time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['week'] = data['date'].dt.week\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['dayofweek'] = data['date'].dt.dayofweek\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2628.57 Mb (60.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "new_fe = False\n",
    "if new_fe:\n",
    "    data = simple_fe(data)\n",
    "    data = reduce_mem_usage(data)\n",
    "    data.to_csv('engineered_data.csv')\n",
    "else:\n",
    "    data = pd.read_csv('engineered_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply hyperopt to select the parameters of the lightGBM\n",
    "1. Define a loss function\n",
    "2. Define the parameter space\n",
    "3. Run Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# define list of features\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1',\n",
    "            'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29',\n",
    "            'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', 'rolling_mean_t180', \n",
    "            'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', \n",
    "            'rolling_skew_t30', 'rolling_kurt_t30']\n",
    "\n",
    "def optimize_parameters(x_train,max_evals=20):\n",
    "    # define fixed hyperparameters\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'subsample': 0.5,\n",
    "        'subsample_freq': 1,\n",
    "        'sub_feature' : 0.8,\n",
    "        'sub_row' : 0.75,\n",
    "        'bagging_freq' : 1,\n",
    "        'lambda_l2' : 0.1,\n",
    "        'verbosity': 1,\n",
    "        'boost_from_average': True,\n",
    "        'n_jobs': -1,\n",
    "        'seed': 3008,\n",
    "        'verbose': -1}\n",
    "    \n",
    "    # define floating hyperparameters\n",
    "    space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 25, 600, 25),\n",
    "        'max_depth': hp.quniform('max_depth', 1, 6, 1),\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 120, 1),\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.01, 0.1, 0.01)\n",
    "    }\n",
    "    \n",
    "    # define the objective function to optimize the hyperparameters\n",
    "    def objective(floating_params):\n",
    "        params['n_estimators'] = int(floating_params['n_estimators'])\n",
    "        params['max_depth'] = int(floating_params['max_depth'])\n",
    "        params['num_leaves'] = int(floating_params['num_leaves'])\n",
    "        params['learning_rate'] = float(floating_params['learning_rate'])\n",
    "        #{'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth']),\n",
    "        #         'num_leaves': int(params['num_leaves'])}\n",
    "        print(params)\n",
    "        regressor = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        x_sample = x_train.sample(int(x_train.shape[0]/50))\n",
    "        x_train_sample, y_train_sample = x_sample[features], x_sample['demand']\n",
    "        \n",
    "        score = cross_val_score(regressor, x_train_sample, y_train_sample, cv=StratifiedKFold(),\n",
    "                                scoring=make_scorer(mean_squared_error, greater_is_better=False)\n",
    "                                ).mean()\n",
    "        print(\"rmse {:.3f} params {}\".format(score, params))\n",
    "        return score\n",
    "\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_evals)\n",
    "    \n",
    "    best['num_iterations'] =  1500\n",
    "    best['n_estimators'] = int(best['n_estimators']); best['max_depth'] = int(best['max_depth']); \n",
    "    best['num_leaves'] = int(best['num_leaves']); best['learning_rate'] = float(best['learning_rate'])\n",
    "    \n",
    "    with open('params.txt', 'w') as file:\n",
    "        file.write(json.dumps(params))\n",
    "    \n",
    "def optimized_lgb(data, update_hyperparameters=False):\n",
    "    \n",
    "    # going to evaluate with the last 28 days\n",
    "    x_train = data[data['date'] <= '2016-03-27']\n",
    "    y_train = x_train['demand']\n",
    "    x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "    y_val = x_val['demand']\n",
    "    test = data[(data['date'] > '2016-04-24')]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    \n",
    "    if update_hyperparameters:\n",
    "        optimize_parameters(x_train)\n",
    "    \n",
    "\n",
    "    train_set = lgb.Dataset(x_train[features], y_train)\n",
    "    val_set = lgb.Dataset(x_val[features], y_val)\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    with open('params.txt') as params_file:    \n",
    "        params = json.load(params_file)\n",
    "    model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, \n",
    "                      valid_sets = [train_set, val_set], verbose_eval = 100)\n",
    "    \n",
    "    val_pred = model.predict(x_val[features])\n",
    "    val_score = np.sqrt(mean_squared_error(val_pred, y_val))\n",
    "    print(f'Our val rmse score is {val_score}')\n",
    "    y_pred = model.predict(test[features])\n",
    "    test['demand'] = y_pred\n",
    "    \n",
    "    return test\n",
    "\n",
    "def append_predictions(test, submission):\n",
    "    predictions = test[['id', 'date', 'demand']]\n",
    "    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on = 'id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    final.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2628.57 Mb (0.0% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's rmse: 2.77088\tvalid_1's rmse: 2.31809\n",
      "[200]\ttraining's rmse: 2.74258\tvalid_1's rmse: 2.27634\n",
      "[300]\ttraining's rmse: 2.72931\tvalid_1's rmse: 2.26119\n",
      "[400]\ttraining's rmse: 2.72355\tvalid_1's rmse: 2.25274\n",
      "[500]\ttraining's rmse: 2.71821\tvalid_1's rmse: 2.24903\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[532]\ttraining's rmse: 2.71663\tvalid_1's rmse: 2.24723\n",
      "Our val rmse score is 2.247227539858133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data = reduce_mem_usage(data)\n",
    "test = optimized_lgb(data,update_hyperparameters=False)\n",
    "append_predictions(test, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
