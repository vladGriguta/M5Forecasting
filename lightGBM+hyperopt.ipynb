{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit to [ragnar](https://www.kaggle.com/ragnar123). This notebook builds upon their data pipeline found in  [ragnar's notebook](https://www.kaggle.com/ragnar123/very-fst-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial notebook:\n",
    "1. Read in data from calendar, sell_prices, sales_train_validation, submission and apply common method to reduce the memory usage\n",
    "2. Merge data into a multivariate time series (MTS) format with each entry in the series describing the target variable (demand) and its exogeneous variables in different dates for different item_id, dept_id, cat_id, store_id and state_id\n",
    "3. Transform features into the MTS (fill NaN events with category 'unknown' and encode all categorical variables)\n",
    "4. Engineer additional features based on the series of prices and demand for each good. Examples include aggregations over lagged sub-series of the two variables\n",
    "### Personal contributions (thus far)\n",
    "5. Hand-crafted parameters for the lightGBM regressor based upon [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)\n",
    "6. Implemented hyper-parameter tuning pipeline for optimizing the n_estimators, max_depth, num_leaves, learning_rate\n",
    "### Future plans\n",
    "7. Find a way to read in the full dataset and avoid memory problems\n",
    "8. Implement a feature selection method (sklearn.feature_selection)\n",
    "9. Engineer additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common function to reduce the memory usage thus allowing us to work with larger datasets\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Common function to reduce the size of the entries in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# simple function to read the data in the competition files\n",
    "def readData(submission_only=False, PATH=''):\n",
    "    import pandas as pd\n",
    "    print('Reading files...')\n",
    "    submission = pd.read_csv(PATH+'m5-forecasting-accuracy/sample_submission.csv')\n",
    "    if submission_only:\n",
    "        return submission\n",
    "    else:\n",
    "        calendar = pd.read_csv(PATH+'m5-forecasting-accuracy/calendar.csv')\n",
    "        calendar = reduce_mem_usage(calendar)\n",
    "        print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "\n",
    "        sell_prices = pd.read_csv(PATH+'m5-forecasting-accuracy/sell_prices.csv')\n",
    "        sell_prices = reduce_mem_usage(sell_prices)\n",
    "        print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "\n",
    "        sales_train_validation = pd.read_csv(PATH+'m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "        print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "\n",
    "    return calendar, sell_prices, sales_train_validation, submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data to get it into a tabular format; pd.melt is especially useful to 'unpack' the target variable (demand)\n",
    "def melt_and_merge():\n",
    "    calendar, sell_prices, sales_train_validation, submission = readData()\n",
    "    # drop some calendar features\n",
    "    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n",
    "    \n",
    "    # melt sales data, get it ready for training\n",
    "    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                                     var_name = 'day', value_name = 'demand')\n",
    "    \n",
    "    #print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "    \n",
    "    # seperate test dataframes\n",
    "    test1_rows = [row for row in submission['id'] if 'validation' in row]\n",
    "    #test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    test1 = submission[submission['id'].isin(test1_rows)]\n",
    "    #test2 = submission[submission['id'].isin(test2_rows)]\n",
    "    \n",
    "    # change column names\n",
    "    test1.columns = ['id'] + ['d_{}'.format(i) for i in range(1914,1942)]\n",
    "    #test2.columns = ['id'] + ['d_{}'.format(i) for i in range(1942,1970)]\n",
    "\n",
    "\n",
    "    # get product table\n",
    "    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "    \n",
    "    # merge with product table\n",
    "    #test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n",
    "    test1 = test1.merge(product, how = 'left', on = 'id')\n",
    "    #test2 = test2.merge(product, how = 'left', on = 'id')\n",
    "    #test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n",
    "    \n",
    "    # \n",
    "    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day',\n",
    "                    value_name = 'demand')\n",
    "    #test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day',\n",
    "    #                value_name = 'demand')\n",
    "    \n",
    "    sales_train_validation = pd.concat([sales_train_validation, test1], axis = 0) # include test2 later\n",
    "    \n",
    "    del test1#, test2\n",
    "    gc.collect()\n",
    "    \n",
    "    # delete test2 for now\n",
    "    #data = data[data['part'] != 'test2']\n",
    "    \n",
    "    sales_train_validation = pd.merge(sales_train_validation, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "    sales_train_validation.drop(['d', 'day'], inplace = True, axis = 1)\n",
    "    \n",
    "    # get the sell price data (this feature should be very important)\n",
    "    sales_train_validation = sales_train_validation.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "    print('Our final dataset to train has {} rows and {} columns'.format(\n",
    "        sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    \n",
    "    del calendar, sell_prices; gc.collect();\n",
    "    \n",
    "    return sales_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function fills up the Nan values and encodes the categorical variables\n",
    "def transform(data):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    # convert to datetime object\n",
    "    data['date'] = pd.to_datetime(data.date)\n",
    "    \n",
    "    # fill NaN features with unknown\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes useful laging features from the target variable and the price\n",
    "# to convert what is initially a sequence to sequence mapping into a regression task (sequence to one),\n",
    "# the author used lagged values starting from the minimum lag of 28 days, which is the forecasting horizon\n",
    "def simple_fe(data):\n",
    "    \n",
    "    # rolling demand features\n",
    "    data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n",
    "    data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n",
    "    data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n",
    "    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "    \n",
    "    \n",
    "    # price features\n",
    "    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) / (data['lag_price_t1'])\n",
    "    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) / (data['rolling_price_max_t365'])\n",
    "    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n",
    "    \n",
    "    # time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['week'] = data['date'].dt.week\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['dayofweek'] = data['date'].dt.dayofweek\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n"
     ]
    }
   ],
   "source": [
    "new_fe = False\n",
    "if new_fe:\n",
    "    data = melt_and_merge()\n",
    "    submission = readData(submission_only=True)\n",
    "    data = transform(data)\n",
    "    print('There are {:e} / {:e} NaN entries in the sell_price column'.format(data.sell_price.isna().sum(),data.shape[0]))\n",
    "    data = simple_fe(data)\n",
    "    data = reduce_mem_usage(data)\n",
    "    data.to_pickle('engineered_data.pkl')\n",
    "else:\n",
    "    data = pd.read_pickle('engineered_data.pkl')\n",
    "    submission = readData(submission_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply hyperopt to select the parameters of the lightGBM\n",
    "1. Define a loss function\n",
    "2. Define the parameter space\n",
    "3. Run Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# define list of features\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1',\n",
    "            'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29',\n",
    "            'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', 'rolling_mean_t180', \n",
    "            'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', \n",
    "            'rolling_skew_t30', 'rolling_kurt_t30']\n",
    "\n",
    "def optimize_parameters(x_train, x_val, max_evals=20):\n",
    "    # define fixed hyperparameters\n",
    "    params = {\n",
    "        'tree_learner':'voting',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'subsample': 0.5,\n",
    "        'subsample_freq': 1,\n",
    "        'sub_feature' : 0.8,\n",
    "        'sub_row' : 0.75,\n",
    "        'bagging_freq' : 1,\n",
    "        'lambda_l2' : 0.1,\n",
    "        'verbosity': 1,\n",
    "        'boost_from_average': True,\n",
    "        'n_jobs': -1,\n",
    "        'learning_rate':0.1,\n",
    "        'seed': 3008,\n",
    "        'verbose': -1}\n",
    "    \n",
    "    # define floating hyperparameters\n",
    "    space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 25, 600, 25),\n",
    "        'max_depth': hp.quniform('max_depth', 1, 6, 1),\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 120, 1),\n",
    "    }\n",
    "    \n",
    "    # define the objective function to optimize the hyperparameters\n",
    "    def objective(floating_params):\n",
    "        params['n_estimators'] = int(floating_params['n_estimators'])\n",
    "        params['max_depth'] = int(floating_params['max_depth'])\n",
    "        params['num_leaves'] = int(floating_params['num_leaves'])\n",
    "\n",
    "        print(params)\n",
    "        regressor = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        x_sample = x_train.sample(int(x_train.shape[0]/50))\n",
    "        x_train_sample, y_train_sample = x_sample[features], x_sample['demand']\n",
    "        train_set = lgb.Dataset(x_train_sample[features], y_train_sample)\n",
    "        \n",
    "        model = lgb.train(params, train_set, num_boost_round = 2500)\n",
    "        val_pred = model.predict(x_val[features])\n",
    "        score = np.sqrt(mean_squared_error(val_pred, x_val['demand']))\n",
    "        \n",
    "        print(\"rmse {:.3f} params {}\".format(score, params))\n",
    "        return score\n",
    "\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_evals)\n",
    "    \n",
    "    best['num_iterations'] =  1500\n",
    "    best['n_estimators'] = int(best['n_estimators']); best['max_depth'] = int(best['max_depth']); \n",
    "    best['num_leaves'] = int(best['num_leaves']);\n",
    "    \n",
    "    with open('params.txt', 'w') as file:\n",
    "        file.write(json.dumps(params))\n",
    "\n",
    "def optimized_lgb(data, update_hyperparameters=False):\n",
    "    \n",
    "    # going to evaluate with the last 28 days\n",
    "    x_train = data[data['date'] <= '2016-03-27']\n",
    "    y_train = x_train['demand']\n",
    "    x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "    y_val = x_val['demand']\n",
    "    test = data[(data['date'] > '2016-04-24')]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    \n",
    "    if update_hyperparameters:\n",
    "        optimize_parameters(x_train, x_val)\n",
    "    \n",
    "\n",
    "    train_set = lgb.Dataset(x_train[features], y_train)\n",
    "    val_set = lgb.Dataset(x_val[features], y_val)\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    with open('params.txt') as params_file:    \n",
    "        params = json.load(params_file)\n",
    "    \n",
    "    model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, \n",
    "                      verbose_eval = 100, valid_sets = [train_set, val_set])\n",
    "    \n",
    "    val_pred = model.predict(x_val[features])\n",
    "    val_score = np.sqrt(mean_squared_error(val_pred, y_val))\n",
    "    print(f'Our val rmse score is {val_score}')\n",
    "    y_pred = model.predict(test[features])\n",
    "    test['demand'] = y_pred\n",
    "    \n",
    "    return test\n",
    "\n",
    "def append_predictions(test, submission):\n",
    "    predictions = test[['id', 'date', 'demand']]\n",
    "    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on = 'id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    final.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2386.86 Mb (0.0% reduction)\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 200, 'max_depth': 3, 'num_leaves': 119}\n",
      "  0%|                                                                           | 0/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.203 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 200, 'max_depth': 3, 'num_leaves': 119}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 325, 'max_depth': 5, 'num_leaves': 107}\n",
      "  5%|██▌                                               | 1/20 [00:08<02:48,  8.87s/trial, best loss: 2.202898480599774]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.150 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 325, 'max_depth': 5, 'num_leaves': 107}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 375, 'max_depth': 4, 'num_leaves': 25}\n",
      " 10%|████▉                                            | 2/20 [00:25<03:20, 11.15s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.172 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 375, 'max_depth': 4, 'num_leaves': 25}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 575, 'max_depth': 2, 'num_leaves': 106}\n",
      " 15%|███████▎                                         | 3/20 [00:41<03:37, 12.78s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.167 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 575, 'max_depth': 2, 'num_leaves': 106}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 175, 'max_depth': 6, 'num_leaves': 75}\n",
      " 20%|█████████▊                                       | 4/20 [00:59<03:48, 14.31s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.180 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 175, 'max_depth': 6, 'num_leaves': 75}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 300, 'max_depth': 3, 'num_leaves': 45}\n",
      " 25%|████████████▎                                    | 5/20 [01:11<03:22, 13.47s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.159 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 300, 'max_depth': 3, 'num_leaves': 45}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 200, 'max_depth': 2, 'num_leaves': 52}\n",
      " 30%|██████████████▋                                  | 6/20 [01:23<03:01, 12.97s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.183 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 200, 'max_depth': 2, 'num_leaves': 52}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 500, 'max_depth': 1, 'num_leaves': 35}\n",
      " 35%|█████████████████▏                               | 7/20 [01:31<02:29, 11.48s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.210 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 500, 'max_depth': 1, 'num_leaves': 35}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 425, 'max_depth': 2, 'num_leaves': 64}\n",
      " 40%|███████████████████▌                             | 8/20 [01:44<02:24, 12.07s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.160 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 425, 'max_depth': 2, 'num_leaves': 64}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 500, 'max_depth': 3, 'num_leaves': 116}\n",
      " 45%|██████████████████████                           | 9/20 [01:59<02:22, 12.98s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.186 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 500, 'max_depth': 3, 'num_leaves': 116}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 400, 'max_depth': 4, 'num_leaves': 62}\n",
      " 50%|████████████████████████                        | 10/20 [02:18<02:28, 14.81s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.183 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 400, 'max_depth': 4, 'num_leaves': 62}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 550, 'max_depth': 4, 'num_leaves': 90}\n",
      " 55%|██████████████████████████▍                     | 11/20 [02:36<02:20, 15.62s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.174 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 550, 'max_depth': 4, 'num_leaves': 90}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 300, 'max_depth': 3, 'num_leaves': 43}\n",
      " 60%|████████████████████████████▊                   | 12/20 [02:58<02:21, 17.70s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.162 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 300, 'max_depth': 3, 'num_leaves': 43}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 225, 'max_depth': 3, 'num_leaves': 67}\n",
      " 65%|███████████████████████████████▏                | 13/20 [03:10<01:51, 15.99s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.159 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 225, 'max_depth': 3, 'num_leaves': 67}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 225, 'max_depth': 6, 'num_leaves': 102}\n",
      " 70%|█████████████████████████████████▌              | 14/20 [03:20<01:24, 14.14s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.169 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 225, 'max_depth': 6, 'num_leaves': 102}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 125, 'max_depth': 1, 'num_leaves': 55}\n",
      " 75%|████████████████████████████████████            | 15/20 [03:34<01:10, 14.18s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.255 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 125, 'max_depth': 1, 'num_leaves': 55}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 500, 'max_depth': 2, 'num_leaves': 115}\n",
      " 80%|██████████████████████████████████████▍         | 16/20 [03:40<00:46, 11.61s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.179 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 500, 'max_depth': 2, 'num_leaves': 115}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 375, 'max_depth': 4, 'num_leaves': 19}\n",
      " 85%|████████████████████████████████████████▊       | 17/20 [03:56<00:38, 12.98s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.156 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 375, 'max_depth': 4, 'num_leaves': 19}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 450, 'max_depth': 1, 'num_leaves': 19}\n",
      " 90%|███████████████████████████████████████████▏    | 18/20 [04:13<00:28, 14.22s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.215 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 450, 'max_depth': 1, 'num_leaves': 19}\n",
      "{'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 275, 'max_depth': 2, 'num_leaves': 28}\n",
      " 95%|█████████████████████████████████████████████▌  | 19/20 [04:28<00:14, 14.24s/trial, best loss: 2.1502202221841205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 2.161 params {'tree_learner': 'voting', 'boosting_type': 'gbdt', 'objective': 'tweedie', 'tweedie_variance_power': 1.1, 'metric': 'rmse', 'subsample': 0.5, 'subsample_freq': 1, 'sub_feature': 0.8, 'sub_row': 0.75, 'bagging_freq': 1, 'lambda_l2': 0.1, 'verbosity': 1, 'boost_from_average': True, 'n_jobs': -1, 'learning_rate': 0.1, 'seed': 3008, 'verbose': -1, 'n_estimators': 275, 'max_depth': 2, 'num_leaves': 28}\n",
      "100%|████████████████████████████████████████████████| 20/20 [04:37<00:00, 13.90s/trial, best loss: 2.1502202221841205]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds.\n",
      "[100]\ttraining's rmse: 2.54277\tvalid_1's rmse: 2.20864\n",
      "[200]\ttraining's rmse: 2.50097\tvalid_1's rmse: 2.18099\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[275]\ttraining's rmse: 2.48489\tvalid_1's rmse: 2.16672\n",
      "Our val rmse score is 2.166719020245881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# eliminate the first 90 datapoints\n",
    "if(data.shape[0]>5e7):\n",
    "    data = data.sort_values('date').iloc[int(2.75e7):]\n",
    "data = reduce_mem_usage(data)\n",
    "test = optimized_lgb(data,update_hyperparameters=True)\n",
    "append_predictions(test, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
