{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit to [ragnar](https://www.kaggle.com/ragnar123). This notebook builds upon their data pipeline found in  [ragnar's notebook](https://www.kaggle.com/ragnar123/very-fst-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial notebook:\n",
    "1. Read in data from calendar, sell_prices, sales_train_validation, submission and apply common method to reduce the memory usage\n",
    "2. Merge data into a multivariate time series (MTS) format with each entry in the series describing the target variable (demand) and its exogeneous variables in different dates for different item_id, dept_id, cat_id, store_id and state_id\n",
    "3. Transform features into the MTS (fill NaN events with category 'unknown' and encode all categorical variables)\n",
    "4. Engineer additional features based on the series of prices and demand for each good. Examples include aggregations over lagged sub-series of the two variables\n",
    "### Personal contributions (thus far)\n",
    "5. Hand-crafted parameters for the lightGBM regressor based upon [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)\n",
    "6. Implemented hyper-parameter tuning pipeline for optimizing the n_estimators, max_depth, num_leaves, learning_rate\n",
    "### Future plans\n",
    "7. Find a way to read in the full dataset and avoid memory problems\n",
    "8. Implement a feature selection method (sklearn.feature_selection)\n",
    "9. Engineer additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common function to reduce the memory usage thus allowing us to work with larger datasets\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Common function to reduce the size of the entries in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# simple function to read the data in the competition files\n",
    "def readData(submission_only=False):\n",
    "    import pandas as pd\n",
    "    print('Reading files...')\n",
    "    if submission_only:\n",
    "        submission = pd.read_csv('m5-forecasting-accuracy/sample_submission.csv')\n",
    "        return submission\n",
    "    else:\n",
    "        calendar = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "        calendar = reduce_mem_usage(calendar)\n",
    "        print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "\n",
    "        sell_prices = pd.read_csv('m5-forecasting-accuracy/sell_prices.csv')\n",
    "        sell_prices = reduce_mem_usage(sell_prices)\n",
    "        print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "\n",
    "        sales_train_validation = pd.read_csv('m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "        print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "\n",
    "    return calendar, sell_prices, sales_train_validation, submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data to get it into a tabular format; pd.melt is especially useful to 'unpack' the target variable (demand)\n",
    "def melt_and_merge():\n",
    "    calendar, sell_prices, sales_train_validation, submission = readData()\n",
    "    # drop some calendar features\n",
    "    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n",
    "    \n",
    "    # melt sales data, get it ready for training\n",
    "    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                                     var_name = 'day', value_name = 'demand')\n",
    "    \n",
    "    #print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "    \n",
    "    # seperate test dataframes\n",
    "    test1_rows = [row for row in submission['id'] if 'validation' in row]\n",
    "    #test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    test1 = submission[submission['id'].isin(test1_rows)]\n",
    "    #test2 = submission[submission['id'].isin(test2_rows)]\n",
    "    \n",
    "    # change column names\n",
    "    test1.columns = ['id'] + ['d_{}'.format(i) for i in range(1914,1942)]\n",
    "    #test2.columns = ['id'] + ['d_{}'.format(i) for i in range(1942,1970)]\n",
    "\n",
    "\n",
    "    # get product table\n",
    "    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "    \n",
    "    # merge with product table\n",
    "    #test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n",
    "    test1 = test1.merge(product, how = 'left', on = 'id')\n",
    "    #test2 = test2.merge(product, how = 'left', on = 'id')\n",
    "    #test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n",
    "    \n",
    "    # \n",
    "    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day',\n",
    "                    value_name = 'demand')\n",
    "    #test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day',\n",
    "    #                value_name = 'demand')\n",
    "    \n",
    "    sales_train_validation = pd.concat([sales_train_validation, test1], axis = 0) # include test2 later\n",
    "    \n",
    "    del test1#, test2\n",
    "    gc.collect()\n",
    "    \n",
    "    # delete test2 for now\n",
    "    #data = data[data['part'] != 'test2']\n",
    "    \n",
    "    sales_train_validation = pd.merge(sales_train_validation, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "    sales_train_validation.drop(['d', 'day'], inplace = True, axis = 1)\n",
    "    \n",
    "    # get the sell price data (this feature should be very important)\n",
    "    sales_train_validation = sales_train_validation.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "    print('Our final dataset to train has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    \n",
    "    del calendar, sell_prices; gc.collect();\n",
    "    \n",
    "    return sales_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function fills up the Nan values and encodes the categorical variables\n",
    "def transform(data):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    # convert to datetime object\n",
    "    data['date'] = pd.to_datetime(data.date)\n",
    "    \n",
    "    # fill NaN features with unknown\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes useful laging features from the target variable and the price\n",
    "# to convert what is initially a sequence to sequence mapping into a regression task (sequence to one),\n",
    "# the author used lagged values starting from the minimum lag of 28 days, which is the forecasting horizon\n",
    "def simple_fe(data):\n",
    "    \n",
    "    # rolling demand features\n",
    "    data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n",
    "    data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n",
    "    data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n",
    "    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "    \n",
    "    \n",
    "    # price features\n",
    "    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) / (data['lag_price_t1'])\n",
    "    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) / (data['rolling_price_max_t365'])\n",
    "    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n",
    "    \n",
    "    # time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['week'] = data['date'].dt.week\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['dayofweek'] = data['date'].dt.dayofweek\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Sell prices has 6841121 rows and 4 columns\n",
      "Sales train validation has 30490 rows and 1919 columns\n"
     ]
    }
   ],
   "source": [
    "new_fe = False\n",
    "if new_fe:\n",
    "    data = melt_and_merge()\n",
    "    _, _, _, submission = readData()\n",
    "    data = transform(data)\n",
    "    print('There are {:e} / {:e} NaN entries in the sell_price column'.format(data.sell_price.isna().sum(),data.shape[0]))\n",
    "    data = simple_fe(data)\n",
    "    data = reduce_mem_usage(data)\n",
    "    data.to_pickle('engineered_data.pkl')\n",
    "else:\n",
    "    data = pd.read_pickle('engineered_data.pkl')\n",
    "    submission = readData(submission_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply hyperopt to select the parameters of the lightGBM\n",
    "1. Define a loss function\n",
    "2. Define the parameter space\n",
    "3. Run Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# define list of features\n",
    "features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1',\n",
    "            'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29',\n",
    "            'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', 'rolling_mean_t180', \n",
    "            'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', \n",
    "            'rolling_skew_t30', 'rolling_kurt_t30']\n",
    "\n",
    "def optimize_parameters(x_train,max_evals=20):\n",
    "    # define fixed hyperparameters\n",
    "    params = {\n",
    "        'tree_learner':'voting',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'subsample': 0.5,\n",
    "        'subsample_freq': 1,\n",
    "        'sub_feature' : 0.8,\n",
    "        'sub_row' : 0.75,\n",
    "        'bagging_freq' : 1,\n",
    "        'lambda_l2' : 0.1,\n",
    "        'verbosity': 1,\n",
    "        'boost_from_average': True,\n",
    "        'n_jobs': -1,\n",
    "        'seed': 3008,\n",
    "        'verbose': -1}\n",
    "    \n",
    "    # define floating hyperparameters\n",
    "    space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 25, 600, 25),\n",
    "        'max_depth': hp.quniform('max_depth', 1, 6, 1),\n",
    "        'num_leaves': hp.quniform('num_leaves', 10, 120, 1),\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.01, 0.1, 0.01)\n",
    "    }\n",
    "    \n",
    "    # define the objective function to optimize the hyperparameters\n",
    "    def objective(floating_params):\n",
    "        params['n_estimators'] = int(floating_params['n_estimators'])\n",
    "        params['max_depth'] = int(floating_params['max_depth'])\n",
    "        params['num_leaves'] = int(floating_params['num_leaves'])\n",
    "        params['learning_rate'] = float(floating_params['learning_rate'])\n",
    "        #{'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth']),\n",
    "        #         'num_leaves': int(params['num_leaves'])}\n",
    "        print(params)\n",
    "        regressor = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        #x_sample = x_train.groupby([['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]).\n",
    "        x_sample = x_train.sample(int(x_train.shape[0]/50))\n",
    "        x_train_sample, y_train_sample = x_sample[features], x_sample['demand']\n",
    "        \n",
    "        score = cross_val_score(regressor, x_train_sample, y_train_sample, cv=StratifiedKFold(),\n",
    "                                scoring=make_scorer(mean_squared_error, greater_is_better=False)\n",
    "                                ).mean()\n",
    "        print(\"rmse {:.3f} params {}\".format(score, params))\n",
    "        return score\n",
    "\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_evals)\n",
    "    \n",
    "    best['num_iterations'] =  1500\n",
    "    best['n_estimators'] = int(best['n_estimators']); best['max_depth'] = int(best['max_depth']); \n",
    "    best['num_leaves'] = int(best['num_leaves']); best['learning_rate'] = float(best['learning_rate'])\n",
    "    \n",
    "    with open('params.txt', 'w') as file:\n",
    "        file.write(json.dumps(params))\n",
    "\n",
    "def optimized_lgb(data, update_hyperparameters=False):\n",
    "    \n",
    "    # do not expect to be able to accurately predict one month in the future for series with less than 90 datapoints\n",
    "    data = data[data.rolling_mean_t90.isna()]\n",
    "    \n",
    "    # going to evaluate with the last 28 days\n",
    "    x_train = data[data['date'] <= '2016-03-27']\n",
    "    y_train = x_train['demand']\n",
    "    x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "    y_val = x_val['demand']\n",
    "    test = data[(data['date'] > '2016-04-24')]\n",
    "    del data\n",
    "    gc.collect()\n",
    "    \n",
    "    if update_hyperparameters:\n",
    "        optimize_parameters(x_train)\n",
    "    \n",
    "\n",
    "    train_set = lgb.Dataset(x_train[features], y_train)\n",
    "    val_set = lgb.Dataset(x_val[features], y_val)\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    with open('params.txt') as params_file:    \n",
    "        params = json.load(params_file)\n",
    "    \n",
    "    \n",
    "    model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, \n",
    "                      verbose_eval = 100, valid_sets = [train_set, val_set])\n",
    "    \n",
    "    val_pred = model.predict(x_val[features])\n",
    "    val_score = np.sqrt(mean_squared_error(val_pred, y_val))\n",
    "    print(f'Our val rmse score is {val_score}')\n",
    "    y_pred = model.predict(test[features])\n",
    "    test['demand'] = y_pred\n",
    "    \n",
    "    return test\n",
    "\n",
    "def append_predictions(test, submission):\n",
    "    predictions = test[['id', 'date', 'demand']]\n",
    "    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on = 'id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    final.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 4458.72 Mb (0.0% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-marius.griguta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input data must be 2 dimensional and non empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b81bf410b2a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimized_lgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mupdate_hyperparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mappend_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-873f6d95ac53>\u001b[0m in \u001b[0;36moptimized_lgb\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, \n\u001b[1;32m--> 105\u001b[1;33m                       verbose_eval = 100, valid_sets = [train_set, val_set])\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_valid_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_sets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36madd_valid\u001b[1;34m(self, data, name)\u001b[0m\n\u001b[0;32m   1730\u001b[0m         _safe_call(_LIB.LGBM_BoosterAddValidData(\n\u001b[0;32m   1731\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             data.construct().handle))\n\u001b[0m\u001b[0;32m   1733\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_valid_sets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    970\u001b[0m                                     \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                                     \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m                                     silent=self.silent, feature_name=self.feature_name, params=self.params)\n\u001b[0m\u001b[0;32m    973\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                     \u001b[1;31m# construct subset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    727\u001b[0m                                                                                              \u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                                                                                              \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m                                                                                              self.pandas_categorical)\n\u001b[0m\u001b[0;32m    730\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_label_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_has_header\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input data must be 2 dimensional and non empty.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'auto'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input data must be 2 dimensional and non empty."
     ]
    }
   ],
   "source": [
    "data = reduce_mem_usage(data)\n",
    "test = optimized_lgb(data,update_hyperparameters=False)\n",
    "append_predictions(test, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
